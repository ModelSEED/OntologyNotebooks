{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure KBase Jupyter Dev Environment\n",
    "<sub><sup>(contact chenry@anl.gov with questions)</sub></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version 3.9.13\n",
      "KBBaseModules 0.0.1\n",
      "Output files printed to:/Users/chenry/workspace/Notebooks//Ontology//sessions/default/output when using KBDevUtils.output_dir\n",
      "modelseedpy 0.3.3\n",
      "cobrakbase 0.3.1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(\"python version \" + platform.python_version())\n",
    "import sys\n",
    "import json\n",
    "from json import dump\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv, concat, set_option\n",
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import shutil\n",
    "import requests\n",
    "from configparser import ConfigParser\n",
    "\n",
    "config = ConfigParser()\n",
    "if not exists(str(Path.home()) + '/.kbase/config'):    \n",
    "    if exists(\"/scratch/shared/code/sharedconfig.cfg\"):\n",
    "        shutil.copyfile(\"/scratch/shared/code/sharedconfig.cfg\",str(Path.home()) + '/.kbase/config')\n",
    "    else:\n",
    "        print(\"You much create a config file in ~/.kbase/config before running this notebook. See instructions: https://docs.google.com/document/d/1fQ6iS_uaaZKbjWtw1MgzqilklttIibNO9XIIJWgxWKo/edit\")\n",
    "        sys.exit(1)\n",
    "config.read(str(Path.home()) + '/.kbase/config')\n",
    "paths = config.get(\"DevEnv\",\"syspaths\").split(\";\")\n",
    "codebase = config.get(\"DevEnv\",\"codebase\",fallback=\"\")\n",
    "for i,filepath in enumerate(paths):\n",
    "    if filepath[0:1] != \"/\":\n",
    "        paths[i] = codebase+\"/\"+filepath\n",
    "sys.path = paths + sys.path\n",
    "\n",
    "from chenry_utility_module.kbdevutils import KBDevUtils\n",
    "kbdevutil = KBDevUtils(\"Ontology\")\n",
    "from modelseedpy import MSPackageManager, MSModelUtil, MSBuilder, MSATPCorrection, MSGapfill, MSGrowthPhenotype, MSGrowthPhenotypes, ModelSEEDBiochem\n",
    "from modelseedpy.core.annotationontology import convert_to_search_role, split_role\n",
    "from modelseedpy.core.mstemplate import MSTemplateBuilder\n",
    "from modelseedpy.helpers import get_template\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "msrecon = kbdevutil.msseedrecon()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "msrecon = kbdevutil.msseedrecon()\n",
    "annoapi = kbdevutil.anno_client(native_python_api=True)\n",
    "\n",
    "#Code for translating obsolete EC numbers\n",
    "with open(kbdevutil.codebase+\"/cb_annotation_ontology_api/data/obsolete_ec.json\") as json_file:\n",
    "    obs_ec = json.load(json_file)\n",
    "\n",
    "def trans_ec(ec):\n",
    "    original_ec = ec\n",
    "    count=0\n",
    "    while ec in obs_ec:\n",
    "        count += 1\n",
    "        if count == 20:\n",
    "            #print(\"Circular reference:\",original_ec,\"->\",ec)\n",
    "            return original_ec\n",
    "        ec = obs_ec[ec]\n",
    "    return ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "# Listing all sessions\n",
    "#print(kbdevutil.list_sessions())\n",
    "# Changing the current session\n",
    "#kbdevutil.set_session(\"published_biolog\")\n",
    "# Printing the current session\n",
    "#print(kbdevutil.session)\n",
    "# Printing current objects\n",
    "print(kbdevutil.list())\n",
    "print(kbdevutil.load(\"model_mapping\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading reaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "biochem = ModelSEEDBiochem.get()\n",
    "\n",
    "filtered_reactions = pd.read_csv(kbdevutil.codebase+\"/cb_annotation_ontology_api/data/FilteredReactions.csv\",sep=\"\\t\")\n",
    "filtered_reaction_hash = {}\n",
    "for [i,row] in filtered_reactions.iterrows():\n",
    "    filtered_reaction_hash[row[\"id\"]] = row[\"reason\"]\n",
    "\n",
    "msrxn_data = {}\n",
    "for rxn in biochem.reactions:\n",
    "    msrxn_data[rxn.id] = {\n",
    "        \"id\":rxn.id,\n",
    "        \"name\":rxn.name,\n",
    "        \"equation\":rxn.build_reaction_string(use_metabolite_names=True),\n",
    "        \"ec\":[],\n",
    "        \"filtered\":None\n",
    "    }\n",
    "    ecnums = rxn.ec_numbers\n",
    "    \n",
    "    for ec in ecnums:\n",
    "        ec = trans_ec(ec)\n",
    "        if ec not in msrxn_data[rxn.id][\"ec\"]:\n",
    "            msrxn_data[rxn.id][\"ec\"].append(ec)\n",
    "\n",
    "    if rxn.id in filtered_reaction_hash:\n",
    "        msrxn_data[rxn.id][\"filtered\"] = filtered_reaction_hash[rxn.id]\n",
    "\n",
    "reaction_ec = pd.read_csv(kbdevutil.config[\"data\"]+\"/ModelSEEDDatabase/Biochemistry/Aliases/Unique_ModelSEED_Reaction_ECs.txt\",sep=\"\\t\")\n",
    "for [i,row] in reaction_ec.iterrows():\n",
    "    if row[\"ModelSEED ID\"] in msrxn_data:\n",
    "        ec = row[\"External ID\"]\n",
    "        ec = trans_ec(ec)\n",
    "        if ec not in msrxn_data[row[\"ModelSEED ID\"]][\"ec\"]:\n",
    "            msrxn_data[row[\"ModelSEED ID\"]][\"ec\"].append(ec)\n",
    "\n",
    "kbdevutil.save(\"msrxn_data\",msrxn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Rhea data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "rhea_data = {}\n",
    "msrxn_data = kbdevutil.load(\"msrxn_data\")\n",
    "#Loading GO terms to get names for rhea IDs because GO has a single reaction resolution that corresponds to Rhea\n",
    "#A problem with this is that not every Rhea ID has a GO term; also, we are assuming the GO mappings are right, and they may not be\n",
    "with open(kbdevutil.codebase+'cb_annotation_ontology_api/data/GO_dictionary.json') as json_file:\n",
    "    go_dictionary = json.load(json_file)\n",
    "\n",
    "#Here I'm also reading in the EC mappings for Rhea so I can get good EC numbers for the Rhea IDs\n",
    "ec_trans = pd.read_csv(kbdevutil.config[\"data\"]+\"/TemplateFunctions/rhea2ec.tsv\",sep=\"\\t\")\n",
    "for [i,row] in ec_trans.iterrows():\n",
    "    row[\"RHEA_ID\"] = str(row[\"RHEA_ID\"])\n",
    "    if row[\"RHEA_ID\"] not in rhea_data:\n",
    "        rhea_data[row[\"RHEA_ID\"]] = {\n",
    "            \"id\":row[\"RHEA_ID\"],\n",
    "            \"ec\":[],\n",
    "            \"name\":None,\n",
    "            \"genes\":[],\n",
    "            \"msrxn\":[]\n",
    "        }\n",
    "    ecnum = trans_ec(row[\"ID\"])\n",
    "    if ecnum not in rhea_data[row[\"RHEA_ID\"]][\"ec\"]:\n",
    "        rhea_data[row[\"RHEA_ID\"]][\"ec\"].append(row[\"ID\"])\n",
    "\n",
    "#Here I load alternative EC mappings for Rhea so I can get good EC numbers for the Rhea IDs\n",
    "ec_trans = pd.read_csv(kbdevutil.config[\"data\"]+\"/TemplateFunctions/rhea-ec-iubmb.tsv\",sep=\"\\t\")\n",
    "for [i,row] in ec_trans.iterrows():\n",
    "    row[\"RHEA_ID\"] = str(row[\"RHEA_ID\"])\n",
    "    if row[\"RHEA_ID\"] not in rhea_data:\n",
    "        rhea_data[row[\"RHEA_ID\"]] = {\n",
    "            \"id\":row[\"RHEA_ID\"],\n",
    "            \"ec\":[],\n",
    "            \"name\":None,\n",
    "            \"genes\":[],\n",
    "            \"msrxn\":[]\n",
    "        }\n",
    "    ecnum = trans_ec(row[\"EC\"])\n",
    "    if ecnum not in rhea_data[row[\"RHEA_ID\"]][\"ec\"]:\n",
    "        rhea_data[row[\"RHEA_ID\"]][\"ec\"].append(ecnum)\n",
    "\n",
    "#Here I use the GO mappings to assign names to the Rhea IDs                                      \n",
    "go_trans = pd.read_csv(kbdevutil.config[\"data\"]+\"/TemplateFunctions/rhea2go.tsv\",sep=\"\\t\")\n",
    "for [i,row] in go_trans.iterrows():\n",
    "    row[\"RHEA_ID\"] = str(row[\"RHEA_ID\"])\n",
    "    if row[\"RHEA_ID\"] not in rhea_data:\n",
    "        rhea_data[row[\"RHEA_ID\"]] = {\n",
    "            \"id\":row[\"RHEA_ID\"],\n",
    "            \"ec\":[],\n",
    "            \"name\":None,\n",
    "            \"genes\":[],\n",
    "            \"msrxn\":[]\n",
    "        }\n",
    "    rhea_data[row[\"RHEA_ID\"]][\"name\"] = row[\"ID\"]\n",
    "    if row[\"ID\"] in go_dictionary[\"term_hash\"]:\n",
    "        rhea_data[row[\"RHEA_ID\"]][\"name\"] += \":\"+go_dictionary[\"term_hash\"][row[\"ID\"]][\"name\"]\n",
    "\n",
    "ms_aliases = pd.read_csv(kbdevutil.codebase+\"/cb_annotation_ontology_api/data/ModelSEED_Reaction_Aliases.txt\",sep=\"\\t\")\n",
    "for [i,row] in ms_aliases.iterrows():\n",
    "    if row[\"Source\"]:\n",
    "        if row[\"External ID\"] in rhea_data:\n",
    "            msrxn = row[\"ModelSEED ID\"]\n",
    "            if msrxn not in rhea_data[row[\"External ID\"]][\"msrxn\"]:\n",
    "                rhea_data[row[\"External ID\"]][\"msrxn\"].append(msrxn)\n",
    "            if msrxn in msrxn_data:\n",
    "                if msrxn_data[msrxn][\"name\"] and not rhea_data[row[\"External ID\"]][\"name\"]:\n",
    "                    rhea_data[row[\"External ID\"]][\"name\"] = msrxn_data[msrxn][\"name\"]\n",
    "                for ec in msrxn_data[msrxn][\"ec\"]:\n",
    "                    ec = trans_ec(ec)\n",
    "                    if ec not in rhea_data[row[\"External ID\"]][\"ec\"]:\n",
    "                        rhea_data[row[\"External ID\"]][\"ec\"].append(ec)\n",
    "\n",
    "#TODO: We need a new more robust naming procedure for Rhea reactions\n",
    "#Step one, if the Rhea is mapped to a ModelSEED reaction with a name, use that name\n",
    "#Step two, if there is no MS rxn or the MS rxn has no name or just and rxn ID or Rhea ID for a name, do the following:\n",
    "#Take the \"activity\" from the first number in the EC number (e.g. 1 = oxidoreductase, 2 = transferase, etc.)\n",
    "#Then print the reactant list and produce list with the activity of the first EC (e.g. Glucose-6-phosphate hydrolase)\n",
    "#Possible explore filtering out obvious cofactors from the lists above (e.g. O2, H+, H2O)\n",
    "\n",
    "kbdevutil.save(\"rhea_data\",rhea_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading SSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "sso_data = {}\n",
    "msrxn_data = kbdevutil.load(\"msrxn_data\")\n",
    "\n",
    "with open(kbdevutil.codebase+\"/cb_annotation_ontology_api/data/SSO_dictionary.json\") as json_file:\n",
    "    sso = json.load(json_file)\n",
    "\n",
    "with open(kbdevutil.codebase+\"/cb_annotation_ontology_api/data/SSO_reactions.json\") as json_file:\n",
    "    sso_rxns = json.load(json_file)\n",
    "\n",
    "for sso_id in sso[\"term_hash\"]:\n",
    "    sso_id = sso_id[4:]\n",
    "    sso_data[sso_id] = {\n",
    "        \"id\":sso_id,\n",
    "        \"name\":sso[\"term_hash\"][\"SSO:\"+sso_id][\"name\"],\n",
    "        \"ec\":[],\n",
    "        \"genes\":[],\n",
    "        \"msrxn\":[],\n",
    "        \"class\":None\n",
    "    }\n",
    "    if sso_id == \"000009137\":\n",
    "        sso_data[sso_id][\"class\"] = \"hypothetical\"\n",
    "    match = re.search(r'(\\d+\\.[\\d-]+\\.[\\d-]+\\.[\\d-]+)',sso[\"term_hash\"][\"SSO:\"+sso_id][\"name\"])\n",
    "    if match:\n",
    "        ec = match.group(0)\n",
    "        ec = trans_ec(ec)\n",
    "        if ec not in sso_data[sso_id][\"ec\"]:\n",
    "            sso_data[sso_id][\"ec\"].append(ec)\n",
    "        \n",
    "for sso_id in sso_rxns:\n",
    "    orig = sso_id\n",
    "    sso_id = sso_id[4:]\n",
    "    if sso_id in sso_data:\n",
    "        for rxn in sso_rxns[orig]:\n",
    "            if rxn not in sso_data[sso_id][\"msrxn\"]:\n",
    "                sso_data[sso_id][\"msrxn\"].append(rxn)\n",
    "            if rxn in msrxn_data:\n",
    "                for ec in msrxn_data[rxn][\"ec\"]:\n",
    "                    if ec not in sso_data[sso_id][\"ec\"]:\n",
    "                        sso_data[sso_id][\"ec\"].append(ec)\n",
    "\n",
    "kbdevutil.save(\"sso_data\",sso_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build KO hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_data = {}\n",
    "msrxn_data = kbdevutil.load(\"msrxn_data\")\n",
    "\n",
    "with open(kbdevutil.codebase+\"/cb_annotation_ontology_api/data/KO_dictionary.json\") as json_file:\n",
    "    kodict = json.load(json_file)\n",
    "\n",
    "for ko in kodict[\"term_hash\"]:\n",
    "    if ko not in ko_data:\n",
    "        ko_data[kodict[\"term_hash\"][ko][\"id\"]] = {\n",
    "            \"id\": kodict[\"term_hash\"][ko][\"id\"],\n",
    "            \"name\": kodict[\"term_hash\"][ko][\"name\"],\n",
    "            \"ec\":[],\n",
    "            \"genes\":[],\n",
    "            \"msrxn\":[]\n",
    "        }\n",
    "        match = re.search(r'(\\d+\\.[\\d-]+\\.[\\d-]+\\.[\\d-]+)',kodict[\"term_hash\"][ko][\"name\"])\n",
    "        if match:\n",
    "            ec = match.group(0)\n",
    "            ec = trans_ec(ec)\n",
    "            if ec not in ko_data[kodict[\"term_hash\"][ko][\"id\"]][\"ec\"]:\n",
    "                 ko_data[kodict[\"term_hash\"][ko][\"id\"]][\"ec\"].append(ec)\n",
    "\n",
    "with open(kbdevutil.codebase+\"/cb_annotation_ontology_api/data/kegg_95_0_ko_seed.tsv\") as f:\n",
    "    korxn = pd.read_csv(f,sep=\"\\t\")\n",
    "\n",
    "for index, row in korxn.iterrows():\n",
    "    if row[\"ko_id\"] in ko_data:\n",
    "        rxns = row[\"seed_ids\"].split(\";\")\n",
    "        for rxn in rxns:\n",
    "            if rxn not in ko_data[row[\"ko_id\"]][\"msrxn\"]:\n",
    "                ko_data[row[\"ko_id\"]][\"msrxn\"].append(rxn)\n",
    "            if rxn in msrxn_data:\n",
    "                for ec in msrxn_data[rxn][\"ec\"]:\n",
    "                    if ec not in ko_data[row[\"ko_id\"]][\"ec\"]:\n",
    "                        ko_data[row[\"ko_id\"]][\"ec\"].append(ec)\n",
    "\n",
    "kbdevutil.save(\"ko_data\",ko_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build EC hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_data = {}\n",
    "\n",
    "with open(kbdevutil.codebase+\"/cb_annotation_ontology_api/data/EC_dictionary.json\") as json_file:\n",
    "    ecdict = json.load(json_file)\n",
    "\n",
    "for ec in ecdict[\"term_hash\"]:\n",
    "    if ec not in ec_data:\n",
    "        ec_data[ecdict[\"term_hash\"][ec][\"id\"]] = {\n",
    "            \"id\": ecdict[\"term_hash\"][ec][\"id\"],\n",
    "            \"name\": ecdict[\"term_hash\"][ec][\"name\"],\n",
    "            \"ec\":[ec],\n",
    "            \"dram_genes\":[],\n",
    "            \"prokka_genes\":[],\n",
    "            \"msrxn\":[],\n",
    "            \"rhea\":[],\n",
    "            \"sso\":[],\n",
    "            \"ko\":[]\n",
    "        }\n",
    "\n",
    "rhea_data = kbdevutil.load(\"rhea_data\")\n",
    "sso_data = kbdevutil.load(\"sso_data\")\n",
    "msrxn_data = kbdevutil.load(\"msrxn_data\")\n",
    "ko_data = kbdevutil.load(\"ko_data\")\n",
    "\n",
    "all_data = {\n",
    "    \"rhea\":rhea_data,\n",
    "    \"sso\":sso_data,\n",
    "    \"msrxn\":msrxn_data,\n",
    "    \"ko\":ko_data\n",
    "}\n",
    "\n",
    "for type in all_data:\n",
    "    for id in all_data[type]:\n",
    "        for ec in all_data[type][id][\"ec\"]:\n",
    "            if ec not in ec_data:\n",
    "                ec_data[ec] = {\"id\":ec,\"name\":None,\"ec\":[ec],\"rhea\":[],\"sso\":[],\"msrxn\":[],\"ko\":[],\"dram_genes\":[],\"prokka_genes\":[]}\n",
    "            ec_data[ec][type].append(id)\n",
    "            \n",
    "kbdevutil.save(\"ec_data\",ec_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling and printing ontology terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1706652651.781481 INFO: get_annotation_ontology_events:{\n",
      "    \"input_ref\": \"154984/SwissProtCuratedProteins.RAST.Prokka.DRAM.Rhea2.glm4ec\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAST-annotate_genome:SSO:2023-08-21T04:51:37\n",
      "Prokka Annotation:3.2.1:ec:2023_08_29_18_26_47\n",
      "DRAM:KO:2023_08_29_20_19_01\n",
      "DRAM:EC:2023_08_29_20_19_01\n",
      "Swissprot Rhea reactions\n",
      "GLM4ECModule.annotate_microbes_with_GLM4EC:0.1.1.am:EC:2024-01-02 20:36:32\n",
      "Term not founds: 19849 in rhea\n",
      "Term not founds: 75299 in rhea\n",
      "Term not founds: 73767 in rhea\n",
      "Term not founds: 73775 in rhea\n",
      "Term not founds: 73779 in rhea\n",
      "Term not founds: 73783 in rhea\n",
      "Term not founds: 73787 in rhea\n",
      "Term not founds: 73791 in rhea\n",
      "Term not founds: 73795 in rhea\n",
      "Term not founds: 73799 in rhea\n",
      "Term not founds: 73803 in rhea\n",
      "Term not founds: 73771 in rhea\n",
      "Term not founds: K24570 in ko\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'genes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ph/360gp1jx5zg7qyqv076prl240000gn/T/ipykernel_14668/3227806113.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mgene\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"genes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                         \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"genes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'genes'"
     ]
    }
   ],
   "source": [
    "#See this object in this narrative: https://narrative.kbase.us/narrative/154984\n",
    "output = annoapi.get_annotation_ontology_events({\n",
    "    \"input_ref\" : \"154984/SwissProtCuratedProteins.RAST.Prokka.DRAM.Rhea2.glm4ec\"\n",
    "})\n",
    "kbdevutil.save(\"swiss_prot_anno\",output)\n",
    "\n",
    "annotations_by_gene = {}\n",
    "for event in output[\"events\"]:\n",
    "    print(event[\"event_id\"])\n",
    "    if event[\"event_id\"][0:4] == \"RAST\":\n",
    "        for gene in event[\"ontology_terms\"]:\n",
    "            if gene not in annotations_by_gene:\n",
    "                annotations_by_gene[gene] = {}\n",
    "            annotations_by_gene[gene][\"sso\"] = event[\"ontology_terms\"][gene]\n",
    "    elif event[\"event_id\"][0:7] == \"DRAM:KO\":\n",
    "        for gene in event[\"ontology_terms\"]:\n",
    "            if gene not in annotations_by_gene:\n",
    "                annotations_by_gene[gene] = {}\n",
    "            annotations_by_gene[gene][\"ko\"] = event[\"ontology_terms\"][gene]\n",
    "    elif event[\"event_id\"][0:7] == \"DRAM:EC\":\n",
    "        for gene in event[\"ontology_terms\"]:\n",
    "            if gene not in annotations_by_gene:\n",
    "                annotations_by_gene[gene] = {}\n",
    "            annotations_by_gene[gene][\"dec\"] = event[\"ontology_terms\"][gene]\n",
    "    elif event[\"id\"][0:4] == \"RHEA\":\n",
    "        for gene in event[\"ontology_terms\"]:\n",
    "            if gene not in annotations_by_gene:\n",
    "                annotations_by_gene[gene] = {}\n",
    "            annotations_by_gene[gene][\"rhea\"] = event[\"ontology_terms\"][gene]\n",
    "    elif event[\"event_id\"][0:6] == \"Prokka\":\n",
    "        for gene in event[\"ontology_terms\"]:\n",
    "            if gene not in annotations_by_gene:\n",
    "                annotations_by_gene[gene] = {}\n",
    "            annotations_by_gene[gene][\"pec\"] = event[\"ontology_terms\"][gene]\n",
    "    elif event[\"event_id\"][0:6] == \"GLM4EC\":\n",
    "        print(\"TEST\")\n",
    "        for gene in event[\"ontology_terms\"]:\n",
    "            if gene not in annotations_by_gene:\n",
    "                annotations_by_gene[gene] = {}\n",
    "            annotations_by_gene[gene][\"glm\"] = event[\"ontology_terms\"][gene]\n",
    "\n",
    "rhea_data = kbdevutil.load(\"rhea_data\")\n",
    "sso_data = kbdevutil.load(\"sso_data\")\n",
    "msrxn_data = kbdevutil.load(\"msrxn_data\")\n",
    "ko_data = kbdevutil.load(\"ko_data\")\n",
    "ec_data = kbdevutil.load(\"ec_data\")\n",
    "all_data = {\n",
    "    \"rhea\":rhea_data,\n",
    "    \"sso\":sso_data,\n",
    "    \"msrxn\":msrxn_data,\n",
    "    \"ko\":ko_data,\n",
    "    \"ec\":ec_data,\n",
    "    \"glm\":ec_data\n",
    "}\n",
    "for gene in annotations_by_gene:\n",
    "    for source in annotations_by_gene[gene]:\n",
    "        for item in annotations_by_gene[gene][source]:\n",
    "            term = item[\"term\"]\n",
    "            term = term.split(\":\")[1]\n",
    "            if source in all_data:\n",
    "                if term in all_data[source]:\n",
    "                    if gene not in all_data[source][term][\"genes\"]:\n",
    "                        all_data[source][term][\"genes\"].append(gene)\n",
    "                else:\n",
    "                    print(\"Term not founds:\",term,\"in\",source)\n",
    "            elif source == \"dec\":\n",
    "                if term in all_data[\"ec\"]:\n",
    "                    if gene not in all_data[\"ec\"][term][\"dram_genes\"]:\n",
    "                        all_data[\"ec\"][term][\"dram_genes\"].append(gene)\n",
    "                else:\n",
    "                    print(\"Term not founds:\",term,\"in\",source)\n",
    "            elif source == \"pec\":\n",
    "                if term in all_data[\"ec\"]:\n",
    "                    if gene not in all_data[\"ec\"][term][\"prokka_genes\"]:\n",
    "                        all_data[\"ec\"][term][\"prokka_genes\"].append(gene)\n",
    "                else:\n",
    "                    print(\"Term not founds:\",term,\"in\",source)\n",
    "            else:\n",
    "                print(\"Unknown source:\",source)\n",
    "\n",
    "kbdevutil.save(\"rhea_data\",rhea_data)\n",
    "kbdevutil.save(\"sso_data\",sso_data)\n",
    "kbdevutil.save(\"msrxn_data\",msrxn_data)\n",
    "kbdevutil.save(\"ko_data\",ko_data)\n",
    "kbdevutil.save(\"ec_data\",ec_data)\n",
    "\n",
    "kbdevutil.save(\"annotations_by_gene\",annotations_by_gene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating domain specific gene lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "domain_specific_lists = {\n",
    "    \"Fungi\" : \"154984/SwissProt_Rhea_Fungi\",\n",
    "    \"Other\" : \"154984/SwissProt_Rhea_Other\",\n",
    "    \"Viridiplantae\" : \"154984/SwissProt_Rhea_Viridiplantae\",\n",
    "    \"Archaea\" : \"154984/SwissProt_Rhea_Archaea\",\n",
    "    \"Bacteria\" : \"154984/SwissProt_Rhea_Bacteria\",\n",
    "    \"Metazoa\" : \"154984/SwissProt_Rhea_Metazoa\"\n",
    "}\n",
    "domain_proteins = {}\n",
    "for domain in domain_specific_lists:\n",
    "    data = kbdevutil.get_object(domain_specific_lists[domain])\n",
    "    for item in data[\"data\"][\"sequences\"]:\n",
    "        domain_proteins[item[\"id\"]] = domain\n",
    "\n",
    "kbdevutil.save(\"domain_proteins\",domain_proteins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print ontology names for symantic comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "annotations_by_gene = kbdevutil.load(\"annotations_by_gene\")\n",
    "\n",
    "all_data = {\n",
    "    \"rhea\":kbdevutil.load(\"rhea_data\"),\n",
    "    \"sso\":kbdevutil.load(\"sso_data\"),\n",
    "    \"msrxn\":kbdevutil.load(\"msrxn_data\"),\n",
    "    \"ko\":kbdevutil.load(\"ko_data\"),\n",
    "    \"dec\":kbdevutil.load(\"ec_data\"),\n",
    "    \"pec\":kbdevutil.load(\"ec_data\")\n",
    "}\n",
    "\n",
    "#TODO: print the domain of each gene (maybe)\n",
    "#TODO: currently this prints a row for each gene - perhaps change to printing a row for each unique combination of two ontology names and list genes instead of a single name\n",
    "\n",
    "records = {\"Term1\":[],\"Term2\":[],\"Name1\":[],\"Name2\":[],\"Source1\":[],\"Source2\":[],\"Gene\":[],\"ReactionMatch\":[],\"ECMatch\":[]}\n",
    "for gene in annotations_by_gene:\n",
    "    sources = list(annotations_by_gene[gene].keys())\n",
    "    for i in range(len(sources)):\n",
    "        for j in range(i+1,len(sources)):\n",
    "            for item in annotations_by_gene[gene][sources[i]]:\n",
    "                for oitem in annotations_by_gene[gene][sources[j]]:\n",
    "                    records[\"Term1\"].append(item[\"term\"])\n",
    "                    records[\"Term2\"].append(oitem[\"term\"])\n",
    "                    records[\"Source1\"].append(sources[i])\n",
    "                    records[\"Source2\"].append(sources[j])\n",
    "                    records[\"Gene\"].append(gene)\n",
    "                    term = item[\"term\"].split(\":\")[1]\n",
    "                    oterm = oitem[\"term\"].split(\":\")[1]\n",
    "                    if term in all_data[sources[i]]:\n",
    "                        name = all_data[sources[i]][term][\"name\"]\n",
    "                        if not name:\n",
    "                            name = term\n",
    "                        if \"ec\" in all_data[sources[i]][term] and len(all_data[sources[i]][term][\"ec\"]) > 0:\n",
    "                            name += \" (\"+\";\".join(all_data[sources[i]][term][\"ec\"])+\")\"\n",
    "                        records[\"Name1\"].append(name)    \n",
    "                        if oterm in all_data[sources[j]]:\n",
    "                            oname = all_data[sources[j]][oterm][\"name\"]\n",
    "                            if not oname:\n",
    "                                oname = oterm\n",
    "                            if \"ec\" in all_data[sources[j]][oterm] and len(all_data[sources[j]][oterm][\"ec\"]) > 0:\n",
    "                                oname += \" (\"+\";\".join(all_data[sources[j]][oterm][\"ec\"])+\")\"\n",
    "                            records[\"Name2\"].append(oname)\n",
    "                            found = \"No\"\n",
    "                            if \"msrxn\" in all_data[sources[i]][term] and len(all_data[sources[i]][term][\"msrxn\"]) > 0:\n",
    "                                for rxn in all_data[sources[i]][term][\"msrxn\"]:\n",
    "                                    if \"msrxn\" in all_data[sources[j]][oterm] and rxn in all_data[sources[j]][oterm][\"msrxn\"]:\n",
    "                                        found = \"Yes\"\n",
    "                                    elif \"msrxn\" not in all_data[sources[j]][oterm] or len(all_data[sources[j]][oterm][\"msrxn\"]) == 0:\n",
    "                                        found = \"No rxn associated with \"+oterm\n",
    "                            else:\n",
    "                                found = \"No rxn associated with \"+term\n",
    "                            records[\"ReactionMatch\"].append(found)\n",
    "                            found = \"No\"\n",
    "                            if \"ec\" in all_data[sources[i]][term] and len(all_data[sources[i]][term][\"ec\"]) > 0:\n",
    "                                for ec in all_data[sources[i]][term][\"ec\"]:\n",
    "                                    if \"ec\" in all_data[sources[j]][oterm] and ec in all_data[sources[j]][oterm][\"ec\"]:\n",
    "                                        found = \"Yes\"\n",
    "                                    elif \"ec\" not in all_data[sources[j]][oterm] or len(all_data[sources[j]][oterm][\"ec\"]) == 0:\n",
    "                                        found = \"No EC associated with \"+oterm\n",
    "                            else:\n",
    "                                found = \"No EC associated with \"+term\n",
    "                            records[\"ECMatch\"].append(found)\n",
    "                        else:\n",
    "                            records[\"Name2\"].append(oterm)\n",
    "                            records[\"ECMatch\"].append(oterm+\" not found\")\n",
    "                            records[\"ReactionMatch\"].append(oterm+\" not found\") \n",
    "                    else:\n",
    "                        if oterm in all_data[sources[j]]:\n",
    "                            oname = all_data[sources[j]][oterm][\"name\"]\n",
    "                            if not oname:\n",
    "                                oname = oterm\n",
    "                            if \"ec\" in all_data[sources[j]][oterm] and len(all_data[sources[j]][oterm][\"ec\"]) > 0:\n",
    "                                oname += \" (\"+\";\".join(all_data[sources[j]][oterm][\"ec\"])+\")\"\n",
    "                            records[\"Name2\"].append(oname)\n",
    "                        else:\n",
    "                            records[\"Name2\"].append(oterm)\n",
    "                        records[\"Name1\"].append(term)\n",
    "                        records[\"ECMatch\"].append(term+\" not found\")\n",
    "                        records[\"ReactionMatch\"].append(term+\" not found\")\n",
    "\n",
    "df = pd.DataFrame.from_dict(records)\n",
    "df.to_csv(kbdevutil.output_dir+\"/annotation_pairs.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing annotation comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "annotations_by_gene = kbdevutil.load(\"annotations_by_gene\")\n",
    "\n",
    "all_data = {\n",
    "    \"rhea\":kbdevutil.load(\"rhea_data\"),\n",
    "    \"sso\":kbdevutil.load(\"sso_data\"),\n",
    "    \"msrxn\":kbdevutil.load(\"msrxn_data\"),\n",
    "    \"ko\":kbdevutil.load(\"ko_data\"),\n",
    "    \"dec\":kbdevutil.load(\"ec_data\"),\n",
    "    \"pec\":kbdevutil.load(\"ec_data\"),\n",
    "    \"domain\":kbdevutil.load(\"domain_proteins\")\n",
    "}\n",
    "\n",
    "translation = {\n",
    "    \"rhea\":\"Rhea\",\n",
    "    \"sso\":\"RAST\",\n",
    "    \"ko\":\"DramKO\",\n",
    "    \"dec\":\"DramEC\",\n",
    "    \"pec\":\"Prokka\",\n",
    "    \"glm\":\"GLM4EC\"\n",
    "}\n",
    "records = {\"Gene\":[],\"Domain\":[],\"Rhea\":[],\"RAST\":[],\"Prokka\":[],\"DramKO\":[],\"DramEC\":[],\"GLM4EC\":[],\"RAST rxn\":[],\"Prokka rxn\":[],\"DramKO rxn\":[],\"DramEC rxn\":[],\"GLM4EC rxn\":[],\"RAST ec\":[],\"Prokka ec\":[],\"DramKO ec\":[],\"DramEC ec\":[],\"GLM4EC ec\":[]}\n",
    "for gene in annotations_by_gene:\n",
    "    records[\"Gene\"].append(gene)\n",
    "    if gene in all_data[\"domain\"]:\n",
    "        records[\"Domain\"].append(all_data[\"domain\"][gene])\n",
    "    else:\n",
    "        records[\"Domain\"].append(\"None\")\n",
    "    all_rhea_rxn = {}\n",
    "    all_rhea_ec = {}\n",
    "    if \"rhea\" in annotations_by_gene[gene]:\n",
    "        for item in annotations_by_gene[gene][\"rhea\"]:\n",
    "            term = item[\"term\"].split(\":\").pop()\n",
    "            if term in all_data[\"rhea\"]:\n",
    "                if \"msrxn\" in all_data[\"rhea\"][term]:\n",
    "                    for rxn in all_data[\"rhea\"][term][\"msrxn\"]:\n",
    "                        all_rhea_rxn[rxn] = 1\n",
    "                if \"ec\" in all_data[\"rhea\"][term]:\n",
    "                    for ec in all_data[\"rhea\"][term][\"ec\"]:\n",
    "                        all_rhea_ec[ec] = 1\n",
    "    for source in translation:\n",
    "        if source in annotations_by_gene[gene]:\n",
    "            name = \"\"\n",
    "            other_ec = {}\n",
    "            other_rxn = {}\n",
    "            rxnmatch = \"No\"\n",
    "            ecmatch = \"No\"\n",
    "            for item in annotations_by_gene[gene][source]:\n",
    "                if len(name) > 0:\n",
    "                    name += \"\\n\"\n",
    "                term = item[\"term\"].split(\":\").pop()\n",
    "                name += term\n",
    "                if term in all_data[source]:\n",
    "                    if all_data[source][term][\"name\"]:\n",
    "                        name += \":\"+all_data[source][term][\"name\"]\n",
    "                    if (len(all_data[source][term][\"ec\"]) > 0):\n",
    "                        name += \"[\"+\";\".join(all_data[source][term][\"ec\"])+\"]\"\n",
    "                        for ec in all_data[source][term][\"ec\"]:\n",
    "                            other_ec[ec] = 1\n",
    "                            if ec in all_rhea_ec:\n",
    "                                ecmatch = \"Yes\"\n",
    "                    for rxn in all_data[source][term][\"msrxn\"]:\n",
    "                        other_rxn[rxn] = 1\n",
    "                        if rxn in all_rhea_rxn:\n",
    "                            rxnmatch = \"Yes\"\n",
    "            records[translation[source]].append(name)\n",
    "            if source != \"rhea\":\n",
    "                if len(all_rhea_rxn) == 0:\n",
    "                    rxnmatch = \"No Rhea rxn\"\n",
    "                if len(all_rhea_ec) == 0:\n",
    "                    rxnmatch = \"No Rhea ec\"\n",
    "                if len(other_rxn) == 0:\n",
    "                    rxnmatch = \"No other rxn\"\n",
    "                if len(other_ec) == 0:\n",
    "                    rxnmatch = \"No other ec\"\n",
    "                records[translation[source]+\" rxn\"].append(rxnmatch)\n",
    "                records[translation[source]+\" ec\"].append(ecmatch)\n",
    "        else:\n",
    "            records[translation[source]].append(\"None\")\n",
    "            if source == \"rhea\":\n",
    "                records[translation[source]+\" rxn\"].append(\"No Rhea\")\n",
    "                records[translation[source]+\" ec\"].append(\"No Rhea\")\n",
    "            else:\n",
    "                records[translation[source]+\" rxn\"].append(\"No function\")\n",
    "                records[translation[source]+\" ec\"].append(\"No function\")\n",
    "df = pd.DataFrame.from_dict(records)\n",
    "df.to_csv(kbdevutil.output_dir+\"/annotation_comparison.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the annotation ontology API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "kbdevutil = KBDevUtils(\"Ontology\",ws_version=\"appdev\")\n",
    "appdev_annoapi = kbdevutil.anno_client(native_python_api=True)\n",
    "with open('debug.json') as json_file:\n",
    "    input_data = json.load(json_file)\n",
    "output = anno_api.add_annotation_ontology_events(input_data)\n",
    "output = anno_api.get_annotation_ontology_events({\n",
    "    \"input_ref\" : \"102004/Methanosarcina_acetivorans_C2A_DRAM_RAST\"\n",
    "#    \"input_ref\" : \"93487/Ruepo_2orMoreRKM\"\n",
    "#    \"input_ref\" : \"77537/Sco_RAST_Prokka_BlastKOALA_PTools_DeepEC_DeepGO\"\n",
    "#    \"input_ref\" : \"77537/Sco_Union_BestUnion_2plus_Best2plus_RASTKEGG\"\n",
    "#    \"input_ref\" : \"77925/Pf5.6\"#,\n",
    "#    \"input_workspace\" : \n",
    "})\n",
    "with open('output.json', 'w') as outfile:\n",
    "    json.dump(output, outfile, indent=2)\n",
    "\n",
    "terms = ontology[\"events\"][0][\"ontology_terms\"]\n",
    "ontology[\"events\"][0][\"ontology_id\"] = \"SEED\"\n",
    "for gene in terms:\n",
    "    terms[gene][0][\"evidence\"] = \"test\"\n",
    "    terms[gene][0][\"term\"] = terms[gene][0][\"term\"].split(\":\")[1]\n",
    "    \n",
    "output = anno_api.add_annotation_ontology_events({\n",
    "    \"input_ref\" : \"GCF_000012265.1\",\n",
    "    \"input_workspace\" : 77925,\n",
    "    \"output_name\" : \"TestOntologyOutput\",\n",
    "    \"events\" : ontology[\"events\"],\n",
    "    \"output_workspace\": \"kimbrel1:narrative_1606152384556\",\n",
    "    \"save\" : 1\n",
    "})\n",
    "\n",
    "ontology = anno_api.get_annotation_ontology_events({\n",
    "    \"input_ref\" : \"TestOntologyOutput\",\n",
    "    \"input_workspace\" : \"kimbrel1:narrative_1606152384556\"\n",
    "})\n",
    "\n",
    "with open('/Users/chenry/output.json', 'w') as outfile:\n",
    "    json.dump(ontology, outfile, indent=2)\n",
    "\n",
    "#Escherichia_coli_K-12_MG1655\n",
    "#Synechocystis_PCC_6803\n",
    "#Methanosarcina_barkeri_Fusaro\n",
    "#Clostridium_beijerinckii_NCIMB_8052\n",
    "#Streptomyces_coelicolor_A3_2\n",
    "\n",
    "ontology_input = {\n",
    "    \"input_ref\":\"Streptomyces_coelicolor_A3_2\",\n",
    "    \"input_workspace\":\"chenry:narrative_1612295985064\",\n",
    "    \"output_name\":\"test\",\n",
    "    \"output_workspace\":\"chenry:narrative_1612295985064\",\n",
    "    \"clear_existing\":0,\n",
    "    \"overwrite_matching\":1,\n",
    "    \"save\":1,\n",
    "    \"events\":[\n",
    "        {\n",
    "            \"event_id\": \"annotate_genome:1.8.1:SSO:2020-11-23T17:51:18\",\n",
    "            \"original_description\": \"annotate_genome:2020-11-23T17:51:18:2020-11-23T17:51:18\",\n",
    "            \"description\": \"annotate_genome:2020-11-23T17:51:18:2020-11-23T17:51:18:2020-11-23T17:51:18\",\n",
    "            \"ontology_id\": \"SSO\",\n",
    "            \"method\": \"annotate_genome\",\n",
    "            \"method_version\": \"1.8.1\",\n",
    "            \"timestamp\": \"2020-11-23T17:51:18\",\n",
    "            \"ontology_terms\":{\"sgl0001\": [{\"term\": \"SSO:000001563\"}]}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "#with open('/Users/chenry/ontology_api_input.json') as json_file:\n",
    "#    ontology_input = json.load(json_file)\n",
    "#print(\"Loading ontology terms to genome!\")\n",
    "output = anno_api.add_annotation_ontology_events(ontology_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Published Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import cobra\n",
    "import cobrakbase\n",
    "kbase_api = cobrakbase.KBaseAPI()\n",
    "\n",
    "genome_list = [\"Sco\",\"Eco\",\"Cbe\",\"Mba\"]\n",
    "pub_model_hash = {\n",
    "    \"Sco\" : \"iMK1208\",\n",
    "    \"Eco\" : \"iML1515.kb\",\n",
    "    \"Cbe\" : \"iCM925_GF\",\n",
    "    \"Mba\" : \"iMG746_GF\"\n",
    "}\n",
    "pub_fba_hash = {\n",
    "    \"Sco\" : \"iMK1208_FBA\",\n",
    "    \"Eco\" : \"iML1515.kb_FBA\",\n",
    "    \"Cbe\" : \"iCM925_FBA\",\n",
    "    \"Mba\" : \"iMG746_FBA\"\n",
    "}\n",
    "pub_pheno_hash = {\n",
    "    \"Sco\" : \"iMK1208_Pheno\",\n",
    "    \"Eco\" : \"iML1515.kb_Pheno\",\n",
    "    \"Cbe\" : \"iCM925_Pheno\",\n",
    "    \"Mba\" : \"iMG746_Pheno\"\n",
    "}\n",
    "stats = {\n",
    "    \"Sco\":{},\"Eco\":{},\"Cbe\":{},\"Mba\":{}\n",
    "}\n",
    "types = [\"Best\",\"Union\",\"RAST\",\"Published\"]\n",
    "entities = [\"gene\",\"reaction\",\"pospheno\"]\n",
    "print(\"Species\\tType\\tReactions\\tGenes\\tGapfilled\\tBlocked\\tPospheno\\tGene match\\tReaction match\\tPheno match\")\n",
    "for genome in genome_list:\n",
    "    #Get:gene associated reactions;genes;gapfilled\n",
    "    models = [genome+\"_Best\",genome+\"_Union\",genome+\"_StdRAST_Mdl\",pub_model_hash[genome]]\n",
    "    count = 0\n",
    "    for model in models:\n",
    "        current_object = kbase_api.get_object(model,\"patrikd:narrative_1605639637696\")\n",
    "        stats[genome][types[count]] = {\n",
    "            \"reactions\":0,\n",
    "            \"gapfilled\":0,\n",
    "            \"blocked\":0,\n",
    "            \"genes\":0,\n",
    "            \"gene_hash\":{},\n",
    "            \"reaction_hash\":{},\n",
    "            \"pospheno\":0,\n",
    "            \"pospheno_hash\":{},\n",
    "            \"match_reaction\":0,\n",
    "            \"match_gene\":0,\n",
    "            \"match_pospheno\":0\n",
    "        }\n",
    "        for rxn in current_object[\"modelreactions\"]:\n",
    "            rxn[\"id\"] = rxn[\"id\"].replace(\"_z0\",\"_c0\")\n",
    "            if \"gapfill_data\" in rxn and len(rxn[\"gapfill_data\"]) > 0:\n",
    "                stats[genome][types[count]][\"gapfilled\"] += 1\n",
    "            elif count == 3 and len(rxn[\"modelReactionProteins\"]) == 0:\n",
    "                stats[genome][types[count]][\"gapfilled\"] += 1\n",
    "            if len(rxn[\"modelReactionProteins\"]) > 0:\n",
    "                stats[genome][types[count]][\"reactions\"] += 1\n",
    "                stats[genome][types[count]][\"reaction_hash\"][rxn[\"id\"]] = 1\n",
    "                for prot in rxn[\"modelReactionProteins\"]:\n",
    "                    for subunit in prot[\"modelReactionProteinSubunits\"]:\n",
    "                        for ftr in subunit[\"feature_refs\"]:\n",
    "                            ftr = ftr.split(\"/\").pop()\n",
    "                            stats[genome][types[count]][\"gene_hash\"][ftr] = 1             \n",
    "        stats[genome][types[count]][\"genes\"] = len(stats[genome][types[count]][\"gene_hash\"])\n",
    "        count += 1\n",
    "    \n",
    "    #Get:blocked\n",
    "    models = [genome+\"_Best_FBA\",genome+\"_Union_FBA\",genome+\"_StdRAST_FBA\",pub_fba_hash[genome]]\n",
    "    count = 0\n",
    "    for model in models:\n",
    "        current_object = kbase_api.get_object(model,\"patrikd:narrative_1605639637696\")\n",
    "        for var in current_object[\"FBAReactionVariables\"]:\n",
    "            if var[\"class\"] == \"Blocked\":\n",
    "                stats[genome][types[count]][\"blocked\"] += 1\n",
    "        count += 1\n",
    "    #Get:Neg;Pos\n",
    "    models = [genome+\"_Best_Pheno\",genome+\"_Union_Pheno\",genome+\"_StdRAST_Pheno\",pub_pheno_hash[genome]]\n",
    "    count = 0\n",
    "    for model in models:\n",
    "        if not (count == 3 and genome == \"Sco\"):\n",
    "            current_object = kbase_api.get_object(model,\"patrikd:narrative_1605639637696\")\n",
    "            for pheno in current_object[\"phenotypeSimulations\"]:\n",
    "                if pheno[\"simulatedGrowth\"] > 0:\n",
    "                    stats[genome][types[count]][\"pospheno_hash\"][pheno[\"id\"]] = 1\n",
    "                    stats[genome][types[count]][\"pospheno\"] += 1\n",
    "        count += 1   \n",
    "    #Computing matches\n",
    "    for entity in entities:\n",
    "        for count in range(0,3):\n",
    "            for entid in stats[genome][\"Published\"][entity+\"_hash\"]:\n",
    "                if entid in stats[genome][types[count]][entity+\"_hash\"]:\n",
    "                    stats[genome][types[count]][\"match_\"+entity] += 1\n",
    "    #Printing results\n",
    "    for currtype in types:\n",
    "        d = stats[genome][currtype]\n",
    "        print(genome+\"\\t\"+currtype+\"\\t\"+str(d[\"reactions\"])+\"\\t\"+str(d[\"genes\"])+\"\\t\"+str(d[\"gapfilled\"])\\\n",
    "            +\"\\t\"+str(d[\"blocked\"])+\"\\t\"+str(d[\"pospheno\"])+\"\\t\"+str(d[\"match_gene\"])+\"\\t\"+str(d[\"match_reaction\"])+\"\\t\"+str(d[\"match_pospheno\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Ontology API Against Gold Standard Genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import cobra\n",
    "import cobrakbase\n",
    "sys.path.append(\"/Users/chenry/code/MetabolicModelGapfilling/lib/\")\n",
    "#sys.path.append(\"/Users/chenry/code/annotation_ontology_api/lib\")\n",
    "from annotation_ontology_api.annotation_ontology_apiServiceClient import annotation_ontology_api\n",
    "#from annotation_ontology_api.annotation_ontology_api import AnnotationOntologyAPI\n",
    "\n",
    "#Test for ontology API\n",
    "kbase_api = cobrakbase.KBaseAPI()\n",
    "#anno_api = AnnotationOntologyAPI({\"data_directory\" : \"/Users/chenry/code/annotation_ontology_api/data/\"},kbase_api.ws_client,None)\n",
    "anno_api = annotation_ontology_api()\n",
    "genome_list = [\"Ani_RAST\"]\n",
    "#genome_list = [\"Sco_RAST\",\"Eco_RAST\",\"Cbe_RAST\",\"Syn_RAST\",\"Mba_RAST\"]\n",
    "genome_hash = {\n",
    "    \"Eco_RAST\": \"Eco_RAST_Prokka\",\n",
    "    \"Cbe_RAST\": \"Cbe_RAST_Prokka\",\n",
    "    \"Syn_RAST\": \"Syn_RAST_Prokka\",\n",
    "    \"Mba_RAST\": \"Mba_RAST_Prokka\",\n",
    "    \"Sco_RAST\": \"Sco_RAST_Prokka_BlastKOALA_PTools_DeepEC_DeepGO\",\n",
    "    \"Ani_RAST\": \"Ani_RAST_Prokka\"\n",
    "}\n",
    "for genome in genome_list:\n",
    "    print(genome)\n",
    "    ontology_output = anno_api.get_annotation_ontology_events({\n",
    "        \"input_ref\" : \"patrikd:narrative_1605639637696/\"+genome,\n",
    "    })\n",
    "    genome_object = kbase_api.get_object(genome,\"patrikd:narrative_1605639637696\")\n",
    "    ontology_input = {\n",
    "        \"input_ref\":genome_hash[genome],\n",
    "        \"input_workspace\":\"patrikd:narrative_1605639637696\",\n",
    "        \"output_name\":genome_hash[genome],\n",
    "        \"output_workspace\":\"patrikd:narrative_1605639637696\",        \n",
    "        \"save\":1,\n",
    "#        \"type\":\"KBaseGenomes.Genome\",\n",
    "#        \"object\":genome,\n",
    "        \"clear_existing\":0,\n",
    "        \"overwrite_matching\":1,\n",
    "        \"events\":[]\n",
    "    }\n",
    "    for event in ontology_output[\"events\"]:\n",
    "        print(event[\"ontology_id\"])\n",
    "        if event[\"ontology_id\"] == \"SSO\":\n",
    "            ontology_input[\"events\"].append(event)\n",
    "            break\n",
    "    \n",
    "    with open('/Users/chenry/output.json', 'w') as outfile:\n",
    "        json.dump(ontology_output, outfile, indent=2)\n",
    "    \n",
    "    if len(ontology_input[\"events\"]) == 1:\n",
    "        print(str(len(ontology_input[\"events\"])))\n",
    "        print(ontology_input[\"events\"][0][\"ontology_id\"])\n",
    "        ontology_output[\"events\"][0][\"method\"] = \"RAST annotation\"\n",
    "        ontology_output[\"events\"][0][\"description\"] = \"RAST annotation:\"+ontology_output[\"events\"][0][\"ontology_id\"]+\":\"+ontology_output[\"events\"][0][\"timestamp\"]    \n",
    "        ontology_output[\"events\"][0][\"ontology_terms\"] = {}\n",
    "        for ftr in genome_object[\"features\"]:\n",
    "            if \"functions\" in ftr:\n",
    "                for func in ftr[\"functions\"]:\n",
    "                    if ftr[\"id\"] not in ontology_input[\"events\"][0][\"ontology_terms\"]:\n",
    "                        ontology_input[\"events\"][0][\"ontology_terms\"][ftr[\"id\"]] = []\n",
    "                    ontology_input[\"events\"][0][\"ontology_terms\"][ftr[\"id\"]].append({\n",
    "                        \"term\": \"SSO:\"+func\n",
    "                    })\n",
    "        for ftr in genome_object[\"cdss\"]:\n",
    "            if \"functions\" in ftr:\n",
    "                for func in ftr[\"functions\"]:\n",
    "                    if ftr[\"id\"] not in ontology_input[\"events\"][0][\"ontology_terms\"]:\n",
    "                        ontology_input[\"events\"][0][\"ontology_terms\"][ftr[\"id\"]] = []\n",
    "                    ontology_input[\"events\"][0][\"ontology_terms\"][ftr[\"id\"]].append({\n",
    "                        \"term\": \"SSO:\"+func\n",
    "                    })\n",
    "        ontology_output = anno_api.add_annotation_ontology_events(ontology_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing SSO reactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing Super Annotated E. coli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/chenry/code/cb_annotation_ontology_api/lib\")\n",
    "import os\n",
    "import cobra\n",
    "import cobrakbase\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "import cplex\n",
    "import optlang\n",
    "import re\n",
    "import pandas as pd\n",
    "from optlang.symbolics import Zero, add\n",
    "import cobra.util.solver as sutil\n",
    "from cobrakbase.core.converters import KBaseFBAModelToCobraBuilder\n",
    "from cobrakbase.Workspace.WorkspaceClient import Workspace as WorkspaceClient\n",
    "from annotation_ontology_api.annotation_ontology_api import AnnotationOntologyAPI\n",
    "from cobra.core.dictlist import DictList\n",
    "from cobra.core import Gene, Metabolite, Model, Reaction\n",
    "from IPython.core.display import HTML\n",
    "#Test for ontology API\n",
    "kbase_api = cobrakbase.KBaseAPI()\n",
    "anno_api = AnnotationOntologyAPI({\"data_directory\" : \"/Users/chenry/code/cb_annotation_ontology_api/data/\"},\n",
    "    kbase_api.ws_client,None)\n",
    "\n",
    "output = anno_api.get_annotation_ontology_events({\n",
    "    \"input_ref\" : \"Eco_Union_BestUnion_2plus_Best2plus_RASTKEGG.pdb\",\n",
    "    \"input_workspace\" : 133085\n",
    "})\n",
    "with open('EcoliSuperAnnotation', 'w') as outfile:\n",
    "    json.dump(output, outfile, indent=2)\n",
    "#Print annotations in tabular form\n",
    "annotations = {}\n",
    "for event in output[\"events\"]:\n",
    "    name = None\n",
    "    if event[\"original_description\"][0:4] == \"RAST\":\n",
    "        name = \"RAST\"\n",
    "    elif event[\"original_description\"][0:6] == \"Prokka\":\n",
    "        name = \"Prokka\"\n",
    "    elif event[\"original_description\"][0:5] == \"Blast\":\n",
    "        name = \"Koala\"\n",
    "    elif event[\"original_description\"][0:7] == \"Pathway\":\n",
    "        name = \"PathwayTools\"\n",
    "    elif event[\"original_description\"][0:6] == \"DeepEC\":\n",
    "        name = \"DeepEC\"\n",
    "    elif event[\"original_description\"][0:6] == \"DeepGO\":\n",
    "        name = \"DeepGO\"\n",
    "    elif event[\"original_description\"][0:3] == \"KBA\":\n",
    "        name = \"PDB\"\n",
    "    if name:\n",
    "        for gene in event[\"ontology_terms\"]:\n",
    "            for item in event[\"ontology_terms\"][gene]:\n",
    "                if \"modelseed_ids\" in item:\n",
    "                    if gene not in annotations:\n",
    "                        annotations[gene] = {}\n",
    "                    for msid in item[\"modelseed_ids\"]:\n",
    "                        if msid not in annotations[gene]:\n",
    "                            annotations[gene][msid] = {}\n",
    "                        if name not in annotations[gene][msid]:\n",
    "                            annotations[gene][msid][name] = []\n",
    "                        if item[\"term\"] not in annotations[gene][msid][name]:\n",
    "                            annotations[gene][msid][name].append(item[\"term\"])\n",
    "#Loading and saving dataframe\n",
    "annos = [\"RAST\",\"Prokka\",\"Koala\",\"PathwayTools\",\"DeepEC\",\"DeepGO\",\"PDB\"]\n",
    "data = {\"Gene\":[],\"Reactions\":[],\"RAST\":[],\"Prokka\":[],\"Koala\":[],\"PathwayTools\":[],\"DeepEC\":[],\"DeepGO\":[],\"PDB\":[]}\n",
    "for gene in annotations:\n",
    "    for rxn in annotations[gene]:\n",
    "        data[\"Gene\"].append(gene)\n",
    "        data[\"Reactions\"].append(rxn)\n",
    "        for anno in annos:\n",
    "            if anno in annotations[gene][rxn]:\n",
    "                data[anno].append(\",\".join(annotations[gene][rxn][anno]))\n",
    "            else:\n",
    "                data[anno].append(None)\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"EcoliSuperAnnotated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anno_api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chenry/code/ProjectNotebooks/TemplateFunctions/OntologyFunctions.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chenry/code/ProjectNotebooks/TemplateFunctions/OntologyFunctions.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ontology \u001b[39m=\u001b[39m anno_api\u001b[39m.\u001b[39mget_annotation_ontology_events({\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenry/code/ProjectNotebooks/TemplateFunctions/OntologyFunctions.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_ref\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39m\"\u001b[39m\u001b[39mPf5.6\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenry/code/ProjectNotebooks/TemplateFunctions/OntologyFunctions.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_workspace\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39m77925\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenry/code/ProjectNotebooks/TemplateFunctions/OntologyFunctions.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m })\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenry/code/ProjectNotebooks/TemplateFunctions/OntologyFunctions.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m/Users/chenry/translation.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m outfile:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenry/code/ProjectNotebooks/TemplateFunctions/OntologyFunctions.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(anno_api\u001b[39m.\u001b[39malias_hash, outfile, indent\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'anno_api' is not defined"
     ]
    }
   ],
   "source": [
    "ontology = anno_api.get_annotation_ontology_events({\n",
    "    \"input_ref\" : \"Pf5.6\",\n",
    "    \"input_workspace\" : 77925\n",
    "})\n",
    "with open('/Users/chenry/translation.json', 'w') as outfile:\n",
    "    json.dump(anno_api.alias_hash, outfile, indent=2)\n",
    "with open('/Users/chenry/output.json', 'w') as outfile:\n",
    "    json.dump(ontology, outfile, indent=2)\n",
    "\n",
    "terms = ontology[\"events\"][0][\"ontology_terms\"]\n",
    "ontology[\"events\"][0][\"ontology_id\"] = \"SEED\"\n",
    "for gene in terms:\n",
    "    terms[gene][0][\"evidence\"] = \"test\"\n",
    "    terms[gene][0][\"term\"] = terms[gene][0][\"term\"].split(\":\")[1]\n",
    "    \n",
    "with open('/Users/chenry/output2.json', 'w') as outfile:\n",
    "    json.dump(ontology, outfile, indent=2)\n",
    "    \n",
    "output = anno_api.add_annotation_ontology_events({\n",
    "    \"input_ref\" : \"GCF_000012265.1\",\n",
    "    \"input_workspace\" : 77925,\n",
    "    \"output_name\" : \"TestOntologyOutput\",\n",
    "    \"events\" : ontology[\"events\"],\n",
    "    \"output_workspace\": \"kimbrel1:narrative_1606152384556\",\n",
    "    \"save\" : 1\n",
    "})\n",
    "\n",
    "#with open('/Users/chenry/genome.json', 'w') as outfile:\n",
    "#    json.dump(output[\"object\"], outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not sure what this code is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "sso_hash = dict()\n",
    "with open('/Users/chenry/Dropbox/workspace/KBase Project/TemplateFunctions/genome_sso.json') as json_file:\n",
    "    sso_hash = json.load(json_file)\n",
    "\n",
    "sso_template = dict()\n",
    "with open('/Users/chenry/Dropbox/workspace/KBase Project/TemplateFunctions/SSO_reactions.json') as json_file:\n",
    "    sso_template = json.load(json_file)\n",
    "\n",
    "reaction_hash = dict()\n",
    "with open('/Users/chenry/Dropbox/workspace/KBase Project/TemplateFunctions/genome_reactions.json') as json_file:\n",
    "    reaction_hash = json.load(json_file)\n",
    "\n",
    "function_hash = dict()\n",
    "with open('/Users/chenry/Dropbox/workspace/KBase Project/TemplateFunctions/genome_functions.json') as json_file:\n",
    "    function_hash = json.load(json_file)\n",
    "\n",
    "functions = dict()\n",
    "comparison = dict()\n",
    "for genome in sso_hash:\n",
    "    if genome in reaction_hash:\n",
    "        sso_based_reactions = dict()\n",
    "        sso_based_genes = dict()\n",
    "        for gene in sso_hash[genome]:\n",
    "            for sso in sso_hash[genome][gene]:\n",
    "                if sso in sso_template:\n",
    "                    for reaction in sso_template[sso]:\n",
    "                        if reaction not in sso_based_reactions:\n",
    "                            sso_based_reactions[reaction] = dict()\n",
    "                        sso_based_reactions[reaction][gene] = 1\n",
    "                        if gene not in sso_based_genes:\n",
    "                            sso_based_genes[gene] = dict()\n",
    "                        sso_based_genes[gene][reaction] = 1\n",
    "        comparison[genome] = {\n",
    "            \"SSO_reactions\": len(sso_based_reactions),\n",
    "            \"SSO_genes\": len(sso_based_genes),\n",
    "            \"Extra_SS_reactions\": [],\n",
    "            \"Extra_SS_genes\": [],\n",
    "            \"Extra_MS_reactions\": [],\n",
    "            \"Extra_MS_genes\": [],\n",
    "            \"Extra_SS_reactions_counts\": 0,\n",
    "            \"Extra_SS_genes_counts\": 0,\n",
    "            \"Extra_MS_reactions_counts\": 0,\n",
    "            \"Extra_MS_genes_counts\": 0,\n",
    "            \"MS_reactions\": len(reaction_hash[genome]),\n",
    "            \"MS_genes\" 0,\n",
    "        }\n",
    "        ms_based_genes = dict()\n",
    "        for reaction in reaction_hash[genome]:\n",
    "            if reaction not in sso_based_reactions:\n",
    "                comparison[genome][\"Extra_MS_reactions\"].append(reaction)\n",
    "                comparison[genome][\"Extra_MS_reactions_counts\"] += 1\n",
    "            for gene in reaction_hash[genome][reaction]:\n",
    "                if gene not in ms_based_genes:\n",
    "                    ms_based_genes[gene] = dict()\n",
    "                ms_based_genes[gene][reaction] = 1\n",
    "        for reaction in sso_based_reactions:\n",
    "            if reaction not in reaction_hash[genome]:\n",
    "                comparison[genome][\"Extra_SS_reactions\"].append(reaction)\n",
    "                comparison[genome][\"Extra_SS_reactions_counts\"] += 1\n",
    "        comparison[genome][\"MS_genes\"] = len(ms_based_genes)\n",
    "        for gene in ms_based_genes:\n",
    "            if gene not in sso_based_genes:\n",
    "                comparison[genome][\"Extra_MS_genes\"].append(gene)\n",
    "                comparison[genome][\"Extra_MS_genes_counts\"] += 1\n",
    "        for gene in sso_based_genes:\n",
    "            if gene not in ms_based_genes:\n",
    "                comparison[genome][\"Extra_SS_genes\"].append(gene)\n",
    "                comparison[genome][\"Extra_SS_genes_counts\"] += 1\n",
    "            \n",
    "with open('/Users/chenry/Dropbox/workspace/KBase Project/TemplateFunctions/comparison.json', 'w') as outfile:\n",
    "    json.dump(comparison, outfile)\n",
    "    \n",
    "with open('/Users/chenry/Dropbox/workspace/KBase Project/TemplateFunctions/problem_functions.json', 'w') as outfile:\n",
    "    json.dump(functions, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing reaction gene associations from all models in workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "objects = msrecon.kbase_api.list_objects(\"chenry:narrative_1581959452634\")\n",
    "reaction_hash = dict()\n",
    "count = 0\n",
    "for obj in objects:\n",
    "    if obj[1][-14:] == \".RAST.mdl.base\":\n",
    "        count += 1\n",
    "        genomeid = obj[1][0:-14]\n",
    "        reaction_hash[genomeid] = dict()\n",
    "        model = kbase.get_from_ws(obj[1],\"chenry:narrative_1581959452634\")\n",
    "        for rxn in model.reactions:\n",
    "            reaction_hash[genomeid][rxn.id.split(\"_\")[0]] = dict()\n",
    "            for prot in rxn.data[\"modelReactionProteins\"]:\n",
    "                for subunit in prot[\"modelReactionProteinSubunits\"]:\n",
    "                    for ftr in subunit[\"feature_refs\"]:\n",
    "                        ftrid = ftr.split(\"/\").pop()\n",
    "                        reaction_hash[genomeid][rxn.id.split(\"_\")[0]][ftrid] = 0\n",
    "\n",
    "with open(kbdevutil.out_dir()+\"genome_reactions.json\", 'w') as outfile:\n",
    "    json.dump(reaction_hash, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
